\documentclass{article}
\usepackage[top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage{fontspec}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{alltt}
\usepackage[utf8]{luainputenc}
\usepackage[bibencoding=utf8,backend=biber]{biblatex}
\addbibresource{cosc6384.final.michael-yantosca.report.bib}
\usepackage{minted}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{pgfgantt}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepgfplotslibrary{external}
\usepgfplotslibrary{statistics}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{pgfplots.groupplots}
\pgfplotsset{
  tick label style={font=\footnotesize},
  label style={font=\small},
  legend style={font=\small},
  compat=newest
}
\pgfplotstableset{
  col sep=comma,
  begin table=\begin{longtable},
  end table=\end{longtable},
  every head row/.append style={after row=\endhead}
}
\usepgflibrary{arrows.meta}
\pagestyle{fancy}
\fancyhf{}
\rhead{Closing the Gap: Optimizing the Gap Enumeration Algorithm}
\lhead{Michael Yantosca}
\rfoot{\thepage}
\newcommand{\todo}[0]{\textbf{\textcolor{red}{TODO}}}
\algloopdefx{ExitWhile}[0]{\textbf{exit while}}
\algloopdefx{NewIf}[1]{\textcolor{red}{\textbf{if} #1 \textbf{then}}}
\algloopdefx{NewEndIf}[0]{\textcolor{red}{\textbf{end if}}}
\algloopdefx{NewElse}[0]{\textcolor{red}{\textbf{else}}}
\algblock{NewIf}{NewEndIf}
%\algblock{NewIf}{NewElse}{NewEndIf}
%\algloopdefx{NewState}[1]{\textcolor{red}{ #1 }}

\pgfplotstableread{./results/taskset-n3-m500-optimization1-1gap.csv}\tasksAa
\pgfplotstableread{./results/taskset-n5-m500-optimization1-1gap.csv}\tasksBa
\pgfplotstableread{./results/taskset-n7-m500-optimization1-1gap.csv}\tasksCa
\pgfplotstableread{./results/taskset-n3-m500-optimization2-1gap.csv}\tasksAb
\pgfplotstableread{./results/taskset-n5-m500-optimization2-1gap.csv}\tasksBb
\pgfplotstableread{./results/taskset-n7-m500-optimization2-1gap.csv}\tasksCb


\begin{document}

\tableofcontents

\begin{section}{Abstract}
  Functional Reactive Programming (FRP) offers an attractive alternative to traditional
  real-time programming paradigms on account of its inherent atomicity by function call,
  which can simplify the process of static analysis through stronger invariants.
  However, these guarantees come at the price of a more complicated timing analysis
  required by the Abort-And-Restart (ANR) scheduling method which Priority-Based FRP
  (P-FRP) employs and which provides the underpinning for the aforementioned atomicity.
  Earlier work by Chaitanya Belwal and Albert M. K. Cheng developed and explored the
  gap-enumeration algorithm, a method for characterizing the worst case response time (WCRT)
  of each task in a set of tasks under the ANR model that outperformed time-accurate simulation
  in terms of efficiency. Here the work is revisited with the aim of further optimizing the
  performance of the gap-enumeration algorithm
\end{section}

\begin{section}{Introduction}
  \begin{paragraph}{}
    The functional programming paradigm has a long history of feeding innovations in other
    programming paradigms, often well ahead of adoption thereof in the latter. For example,
    anonymous lambda closures and first-class function passing, which were not available
    in the C++ language standard until version 11\autocite{CPPLambdas}, have been a staple of languages such
    as LISP since at least the 1970s\autocite[2]{HistoryOfHaskell}. The use of recursion in functional programming languages
    makes inductive proofs of program correctness much simpler than similar iterative
    constructions in imperative languages, particularly in languages that allow
    side-effects by default. It is easier to reason about idempotent functions than
    procedures with the potential for side-channel input via shifting program state.
  \end{paragraph}
  \begin{paragraph}{}
    It raises the question then as to why the field of real-time systems programming has
    not more readily adopted the functional programming paradigm. The answer lies in
    the division between application-level programming and systems-level programming.
    While functional programming at the application level can fuel rapid application
    development, implementing runtime systems that enable such programming is a difficult
    and complicated task. The software transactional memory model described by
    Simon Peyton Jones in his essay ``Beautiful Concurrency''\autocite{BeautifulConcurrency} can still fall
    victim to thread starvation\autocite{TQueueBug}, and the stateless idempotency and atomicity
    guarantees of function execution ultimately have to be implemented on stateful transistors.
  \end{paragraph}
  \begin{paragraph}{}
    In an age of increasing systems complexity, it behooves practitioners of real-time systems
    programming to investigate whether systems can be built to employ functional programming
    techniques that might alleviate the complexity of the necessary systems analysis.
    Some research has been done in this area, but one of the primary obstacles to analysis
    is the required scheduling model. While much of real-time systems scheduling research
    has focused on preemptive scheduling since Liu and Layland's seminal paper\autocite{LiuLayland1973},
    the guarantees of functional execution atomicity in P-FRP scheduling requires abort-and-restart
    scheduling\autocite[1]{BelwalCheng}.
  \end{paragraph}
  \begin{paragraph}{}
    Characterizing the ANR model is expensive and difficult, as indicated by Belwal
    and Cheng\autocite[2]{BelwalCheng}. However, if the characterization can be made less
    expensive, this in turn can facilitate more research into the adoption of functional
    programming techniques in real-time systems programming, which can provide a foundation
    for building safer, more reliable systems in a more systematic manner. The following work
    attempts to optimize the original work by Belwal and Cheng to provide an efficient
    worst case response time determination for the lowest priority task in a P-FRP task set.
    The terms and variables used follow the original work\autocite[3-4]{BelwalCheng} except that the
    task period is denoted $P$ and in cases where terms and variables are otherwise explicitly specified.
  \end{paragraph}
\end{section}

\begin{section}{Methodology}
  \begin{subsection}{Implementing the Original Algorithm}
    \begin{paragraph}{}
      The original gap-enumeration algorithm is given below with some slight modifications
      for clarity.

    \begin{algorithm}[H]
      \caption{Gap-Enumeration Algorithm\autocite[11]{BelwalCheng}}\label{gapenum1}
      \begin{algorithmic}[1]
        \Function{Gap-Enumerate-Dynamic}{$\Gamma_{n}$, $\tau_{j}$, $w$}
          \State $L \gets \lceil\frac{P_{j}}{w}\rceil$
          \State $U \gets P_{j} + \lceil\frac{P_{j}}{w}\rceil$
          \While{$L < U$}
            \State $\sigma_{n}(P|_{0}^{L}) \gets \{[0,L)\}$
            \For{$i \gets n-1,j$}
              \State $\sigma_{i-1}(P|_{0}^{L}) \gets \Call{Gap-Transform}{L, \sigma_{i}(P|_{0}^{L}), \Gamma_{n}, j}$
              \If{$\sigma_{i-1}(P|_{0}^{L}) = \emptyset$}
                \State \Return -1
              \EndIf
            \EndFor
            \State $[t_{1},t_{2}) \gets \Call{Gap-Search}{\sigma_{j}(P|_{0}^{L}), C_{j}}$
            \If{$t_{1} \geq 0$}
              \State $RT_{j} \gets t_{1} + C_{j}$
            \EndIf
            \If{$RT_{j} < P_{j}$}
              \State \Return $RT_{j}$
            \EndIf
            \State $L \gets L + \lceil\frac{P_{j}}{w}\rceil$
          \EndWhile
        \State \Return -1
      \EndFunction
      \end{algorithmic}
    \end{algorithm}

    The original work set the post-search $t_{1}$ validity test as $t_{1} > 0$, but
    this precludes scheduling work at $t = 0$, so the test $t_{1} \geq 0$
    was substituted.
    \end{paragraph}

    \begin{paragraph}{}
      The original gap-transformation algorithm is given below with some slight
      modifications for clarity as necessitated by implementation details.

    \begin{algorithm}[H]
      \caption{Gap-Tranformation Algorithm\autocite[12]{BelwalCheng}}\label{gapxfrm1}
      \begin{algorithmic}[2]
        \Function{Gap-Transform}{$W$, $\sigma_{i}(P|_{0}^{L})$, $\Gamma_{n}$, $j$}
          \State $J_{i} \gets \lceil\frac{W - R_{j}}{P_{j}}\rceil$
          \For{$q \gets 1, J_{i}$}
            \State $t \gets R_{j} + P_{j}(q-1)$
            \State $kgap \gets \Call{Min-Gap}{\sigma_{i}(P|_{0}^{L})}$
            \State $t_{1} \gets \Call{Entry}{kgap}$
            \State $t_{2} \gets \Call{Exit}{kgap}$
            \While{$kgap \not= \Call{Nil}{\sigma_{i}(P|_{0}^{L})}$}
              \If{$t_{1} > t + P_{j}$}
                \State \Return $\emptyset$
              \EndIf
              \If{$t < t_{1}$}
                \State $t \gets t_{1}$
              \EndIf
              \If{$t_{1} \leq t$ and $t < t_{2}$}
                \State \Call{Gap-Delete}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t_{2})$}
                \If{$t + C_{j} = t_{2}$}
                  \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                  \ExitWhile
                \EndIf
                \If{$t + C_{j} < t_{2}$}
                  \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                  \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t + C_{j},t_{2})$}
                  \ExitWhile
                \EndIf
                \If{$t + C_{j} > t_{2}$}
                  \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                \EndIf
              \EndIf
              \If{$t1 = \Call{Entry}{kgap}$}
                \State $kgap \gets \Call{Successor-Gap}{t_{1}}$
              \EndIf
            \EndWhile
          \EndFor
          \State $\sigma_{i-1}(P|_{0}^{L}) \gets \sigma_{i}(P|_{0}^{L})$
          \State \Return $\sigma_{i-1}(P|_{0}^{L})$
        \EndFunction
      \end{algorithmic}
    \end{algorithm}
    \end{paragraph}
    \begin{paragraph}{}
      Note that the test before performing an advance along the tree is required
      since the reinserted node whose entry time is $t + C_{j}$ in the case where
      the $k$-gap is filled with slack at the end must not be skipped in the gap traversal.
    \end{paragraph}
  \end{subsection}

  \begin{subsection}{Task Set Generation}
    \begin{paragraph}{}
      Task set generation generally followed the parameters outlined in the original work\autocite[14]{BelwalCheng}.
      Tasks were randomly generated in 3 groups ($A$, $B$, and $C$) where group $A$ was
      comprised of sets with 3 tasks, group $B$ with sets of 5 tasks, and group $C$ with
      sets of 7 tasks. All tasks were implicit-deadline and implicit-priority, i.e.,
      the deadline was implicitly defined as equal to the task release period,
      and the priority was implicitly defined as the task period with higher priority
      given to tasks with smaller periods. The prioritization of tasks in this way is
      required in order for the gap-enumeration algorithm to work and correctly model
      the behavior of the P-FRP scheduler.
    \end{paragraph}
    \begin{paragraph}{}
      Task periods were randomly distributed over a uniform distribution within the
      interval ${[}40,60{)}$ by means of the \texttt{std::uniform\_int\_distribution}
      facility of the C++ language\autocite{stduniformintdist}. The distribution
      was generated by of the
      \texttt{std::shuffle\_order\_engine}\autocite{stdshuffleorderengine} pseudo-random
      number generator (PRNG) with the parameters of the default provided \texttt{knuth\_b}
      instance seeded by the nanosecond epoch time immediately prior to the instantiation
      of the PRNG.
    \end{paragraph}
    \begin{paragraph}{}
      Task computation times were randomly distributed over the interval ${[}4,10{)}$
      in the same manner using an independent distribution and PRNG. The distributions
      were queried over a series of iterations of two nested loops, the outer loop
      spanning the population of task sets and the inner loop spanning the task set size.
      As each task set was constructed, the tasks were sorted in increasing order by
      priority, i.e., most urgent and frequent task first, with ties broken by
      computation time in decreasing order thereof.
    \end{paragraph}
    \begin{paragraph}{}
      Within a task set population, each task set was unique in terms of the task
      set serialization. This is to say that the shell command
      \begin{alltt}
        wc -l \$TASKSETFILE
      \end{alltt}
      yielded the same results as
      \begin{alltt}
        sort \$TASKSETFILE | uniq | wc -l
      \end{alltt}
      Since each task set was deterministically sorted first by priority and then by
      computation time, the uniqueness of the string serialization of the task set is
      equivalent to the uniqueness of the task set itself.
    \end{paragraph}
  \end{subsection}

  \begin{subsection}{Optimization 1}
    \begin{paragraph}{}
      Having implemented the original algorithm, it became clear that the constant
      deletion and reinsertion costs were a major component of the algorithm's
      computational cost, validating conclusions drawn in the original work\autocite[17]{BelwalCheng}. However, rather than devising a space-costly indexing scheme to try and speed
      up the tree modification operations, it was determined that a less expensive
      adjustment might yield even better efficiency after observing that the original
      algorithm reinserted a $k$-gap of size zero (0) back into the tree after a job
      had been scheduled into a gap either for a completed or aborted execution.
      Since these null gaps could never fit a task in following iterations of the
      gap-transformation function, including them in the tree incurred extraneous costs.
    \end{paragraph}
    \begin{paragraph}{}
      Consequently, modifications were made to perform a size check
      on $k$-gaps prior to reinsertion to prevent null gaps from being introduced
      as outlined in the following algorithm. The changes in the algorithm are
      highlighted in red for legibility.
    \begin{algorithm}[H]
      \caption{Gap-Tranformation Algorithm Optimization 1: No Zero Gaps Reinserted}\label{gapxfrm2}
      \begin{algorithmic}[2]
        \Function{Gap-Transform-Positive}{$W$, $\sigma_{i}(P|_{0}^{L})$, $\Gamma_{n}$, $j$}
          \State $J_{i} \gets \lceil\frac{W - R_{j}}{P_{j}}\rceil$
          \For{$q \gets 1, J_{i}$}
            \State $t \gets R_{j} + P_{j}(q-1)$
            \State $kgap \gets \Call{Min-Gap}{\sigma_{i}(P|_{0}^{L})}$
            \State $t_{1} \gets \Call{Entry}{kgap}$
            \State $t_{2} \gets \Call{Exit}{kgap}$
            \While{$kgap \not= \Call{Nil}{\sigma_{i}(P|_{0}^{L})}$}
              \If{$t_{1} > t + P_{j}$}
                \State \Return $\emptyset$
              \EndIf
              \If{$t < t_{1}$}
                \State $t \gets t_{1}$
              \EndIf
              \If{$t_{1} \leq t$ and $t < t_{2}$}
                \State \Call{Gap-Delete}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t_{2})$}
                \If{$t + C_{j} = t_{2}$}
                  \NewIf{$t > t_{1}$}
                    \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                  \NewEndIf
                  \ExitWhile
                \EndIf
                \If{$t + C_{j} < t_{2}$}
                    \NewIf{$t > t_{1}$}
                      \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                    \NewEndIf
                  \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t + C_{j},t_{2})$}
                  \ExitWhile
                \EndIf
                \If{$t + C_{j} > t_{2}$}
                  \NewIf{$t > t_{1}$}
                    \State \Call{Gap-Insert}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                  \NewEndIf
                \EndIf
              \EndIf
              \If{$t1 = \Call{Entry}{kgap}$}
                \State $kgap \gets \Call{Successor-Gap}{t_{1}}$
              \EndIf
            \EndWhile
          \EndFor
          \State $\sigma_{i-1}(P|_{0}^{L}) \gets \sigma_{i}(P|_{0}^{L})$
          \State \Return $\sigma_{i-1}(P|_{0}^{L})$
        \EndFunction
      \end{algorithmic}
    \end{algorithm}
    \end{paragraph}
  \end{subsection}

  \begin{subsection}{Optimization 2}
    \begin{paragraph}{}
      Further examination of the original algorithm led to the conclusion that the
      algorithm was not making full utilization of the properties of the red-black tree
      data structure.
      Each iteration of the gap-transformation function had to find the minimum leaf
      at the start, an $O(h)$ operation from the root of a tree of size $h$\autocite[246-249]{CLR}.
      Given that the height of a red-black tree of size $n$ is a maximum of $O(log_{2}(n))$\autocite[263-265]{CLR},
      the asymptotic cost is thus $O(log_{2}(n))$ just to start a transform iteration.
    \end{paragraph}
    \begin{paragraph}{}
      The sequential walk over the $k$-gaps in the tree meant that every iteration also
      incurred an $O(n)$ cost. Within each iteration was the potential for $O(log_{2}(n))$
      insertion and deletion costs\autocite[268-277]{CLR}, as well as another possible
      $O(log_{2}(n)$ operation to find the successor $k$-gap.
    \end{paragraph}
    \begin{paragraph}{}
      In order to avoid these compounding costs, the next optimization preserved the
      prohibition on reinsertion of null gaps and changed the underlying data structure
      to a linked list. By doing this, the start of a transform iteration became a
      constant-time operation since the list maintains a pointer to its own head.
      To simplify operations, a nil sentinel was also maintained at the tail.
      The transform iteration walk remained $O(n)$, but this was unavoidable since
      the nature of the ANR scheduling model demanded that gaps be filled with
      aborted jobs until the job reached completion or the gaps were exhausted.
      The insertion and deletion operations became constant-time since the gaps
      were simply spliced in and spliced out, respectively. In some cases, no pointer
      adjustments were even required since the existing $k$-gap could simply be modified
      \emph{in situ} with regard to its entry and exit.
    \end{paragraph}
    \begin{algorithm}[H]
      \caption{Gap-Tranformation Algorithm Optimization 2: No Zero Gaps Reinserted, Linked List}\label{gapxfrm2}
      \begin{algorithmic}[2]
        \Function{Gap-Transform-Positive-List}{$W$, $\sigma_{i}(P|_{0}^{L})$, $\Gamma_{n}$, $j$}
          \State $J_{i} \gets \lceil\frac{W - R_{j}}{P_{j}}\rceil$
          \For{$q \gets 1, J_{i}$}
            \State $t \gets R_{j} + P_{j}(q-1)$
            \State $kgap \gets \Call{Head}{\sigma_{i}(P|_{0}^{L})}$
            \State $t_{1} \gets \Call{Entry}{kgap}$
            \State $t_{2} \gets \Call{Exit}{kgap}$
            \While{$kgap \not= \Call{Tail}{\sigma_{i}(P|_{0}^{L})}$}
              \If{$t_{1} > t + P_{j}$}
                \State \Return $\emptyset$
              \EndIf
              \If{$t < t_{1}$}
                \State $t \gets t_{1}$
              \EndIf
              \If{$t_{1} \leq t$ and $t < t_{2}$}
                \If{$t + C_{j} \geq t_{2}$}
                  \If{$t > t_{1}$}
                    \State {$[t_{1},t_{2}) \gets [t_{1},t)$}
                  \Else
                    \State {\Call{Splice-Out}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t_{2})$}}
                  \EndIf
                  \If{$t + C_{j} = t_{2}$}
                    \ExitWhile
                  \EndIf
                \EndIf
                \If{$t + C_{j} < t_{2}$}
                    \State {$[t_{1},t_{2}) \gets [t + C_{j},t_{2})$}
                    \If{$t > t_{1}$}
                      \State \Call{Splice-In}{$\sigma_{i}(P|_{0}^{L}), [t_{1},t)$}
                    \EndIf
                  \ExitWhile
                \EndIf
              \EndIf
              \If{$t1 = \Call{Entry}{kgap}$}
                \State $kgap \gets \Call{Next}{t_{1}}$
              \EndIf
            \EndWhile
          \EndFor
          \State $\sigma_{i-1}(P|_{0}^{L}) \gets \sigma_{i}(P|_{0}^{L})$
          \State \Return $\sigma_{i-1}(P|_{0}^{L})$
        \EndFunction
      \end{algorithmic}
      \end{algorithm}
  \end{subsection}
  \begin{subsection}{Metrics}
    \begin{paragraph}{}
      As with the original work, determining the number of computational steps required for
      each implementation was a desired metric. However, unlike the original work, which
      added a blanket $O(log(n))$ cost to every red-black tree insertion and deletion\autocite[14]{BelwalCheng},
      empirical computational steps were counted within the major loops and operations
      inside the various functions.
    \end{paragraph}
    \begin{paragraph}{}
      For the red-black tree versions, this included
      \textsc{Gap-Enumerate-Dynamic}, \textsc{Gap-Transform} or \textsc{Gap-Transform-Positive},
      and \textsc{Gap-Search}, as well as \textsc{RB-Insert}, \textsc{RB-Delete},
      \textsc{Tree-Successor}, and \textsc{Tree-Minimum}. For the linked-list
      version, this included \textsc{Gap-Enumerate-Dynamic}, \textsc{Gap-Transform-Positive-List},
      and \textsc{Gap-Search}, as well as \textsc{Splice-In} and \textsc{Splice-Out}.
      The \textsc{Head}, \textsc{Tail}, and \textsc{Next} functions were not included
      since they represent simple pointer accesses.
    \end{paragraph}
    \begin{paragraph}{}
      For each run of the gap-enumeration algorithm, the number of $k$-gaps present in
      the final tree or list was also counted as a measure of space complexity.
      Wall-clock execution time was measured with the POSIX system clock \texttt{CLOCK\_MONOTONIC}.
    \end{paragraph}
  \end{subsection}
\end{section}

\begin{section}{Results}

  \begin{subsection}{Original Method vs. Optimization 1}
    % Computational Steps
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=Task Set Number,
          ylabel=Computational Steps,
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksAa};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksBa};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksCa};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      As expected, the prohibition on null $k$-gaps in the gap tree significantly
      reduced the number of computational steps required for gap enumeration.
      The range of computational steps required for Optimization 1 has a fair
      amount of jitter, probably owing to the nature of the exact tree structure
      as informed by the task set definition and the interplay between the various
      tasks in the ANR scheduling model. The variance is most likely a function
      of the red-black tree insertion and deletion operations.
    \end{paragraph}

    % Execution Time
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=Task Set Number,
          ylabel=Execution Time ({\mu}s),
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksAa};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksBa};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksCa};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      The results exhibit a decent speedup in wall-clock execution time.
      However, these results must be considered in the context of
      the testing platform as the exact execution time speedup will
      depend on the hardware and operating system being used. The computational
      step measurement is more informative from a hardware-agnostic, purely
      algorithmic standpoint.
    \end{paragraph}

    % Computational Steps
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=1-Gaps,
          ylabel=Computational Steps,
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksAa};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksBa};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 1},
          legend to name=version
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksCa};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      The results here clearly delineate the drop in space complexity induced
      by not reinserting the null $k$-gaps and the subsequent drop in empirical
      time complexity. The gap between the cost clusters increases proportional to the
      task set size.
    \end{paragraph}
  \end{subsection}

  \begin{subsection}{Original Method vs. Optimization 2}

    % Computational Steps
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=Task Set Number,
          ylabel=Computational Steps,
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksAb};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksBb};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=steps,meta=version] {\tasksCb};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version2}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      As expected, the linked-list implementation significantly
      reduced the number of computational steps required for gap enumeration.
      The range of computational steps required for Optimization 2 has less
      jitter than Optimization 2 since the insertion and deletion costs were
      constant. Considering the previous graphs, Optimization 2 slightly
      outperforms Optimization 1 in general.
    \end{paragraph}

    % Execution Time
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=Task Set Number,
          ylabel=Execution Time ({\mu}s),
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksAb};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksBb};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=m,y=μs,meta=version] {\tasksCb};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version2}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      The results for Optimization 2 exhibit a decent speedup in wall-clock execution time.
      There seems to be an anomaly with regard to the execution time of task
      sets listed earlier in the population. The exponential decay as population identifier
      increases likely stems either from whole program initialization costs or from
      interference due to external, transient load on the platform under test.
      Further experiments could validate or falsify these hypotheses.
    \end{paragraph}


    % 1-Gaps vs. Computational Steps
    \begin{paragraph}{}
    \begin{tikzpicture}
      \begin{groupplot}[
          group style={
            group size=3 by 1,
            xlabels at=edge bottom,
            ylabels at=edge left
          },
          width=2in,
          height=2in,
          xlabel=1-Gaps,
          ylabel=Computational Steps,
          legend style={legend columns=-1}
        ]
        \nextgroupplot[
          title={Task Set Group A ($n = 3$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksAb};
        \nextgroupplot[
          title={Task Set Group B ($n = 5$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksBb};
        \nextgroupplot[
          title={Task Set Group C ($n = 7$)},
          scatter, only marks, scatter src=explicit symbolic,
          scatter/classes={
            gapenum1={mark=o,blue},
            gapenum2={mark=o,red}
          },
          legend entries={Original, Optimization 2},
          legend to name=version2
        ]
        \addplot table [x=gaps,y=steps,meta=version] {\tasksCb};
      \end{groupplot}
      \node at (4,-1.5) {\pgfplotslegendfromname{version2}};
    \end{tikzpicture}
    \end{paragraph}

    \begin{paragraph}{}
      The results here reproduce the effect seen from the first optimization
      with regard to space and time complexity. The improvement over the first
      optimization is clearly visible in the task sets from group $C$.
    \end{paragraph}
  \end{subsection}
\end{section}

\begin{section}{Conclusions}
  \begin{paragraph}{}
    Avenues for future work include more comprehensive tests with different types of task
    sets. Although Belwal and Cheng illustrated the ANR scheduling model and the gap
    enumeration algorithm succinctly with a task set whose task periods varied greatly
    within the set, the experiments run both in the original work and the present work
    covered fairly homogeneous task sets by contrast. The homogeneity of the task sets
    tested does not fully explore the pathology of the worst case scenario for the
    algorithm and the underlying data structures, which reach their nadir in task sets
    where the higher priority tasks are frequently released within the search hyperperiod,
    thereby creating many disjoint $k$-gaps which must be walked by lower priority tasks
    in search of an execution space.
  \end{paragraph}
  \begin{paragraph}{}
    No effort was made in this work to improve the gap-search algorithm, which remains
    an $O(n)$ operation. Future work could focus on some form of efficient meta-indexing
    to reduce the search time. Ideally, this would be built during the gap-transformation
    function to amortize the time costs required for such indexing.
    Alternatively, investigations into a more flexible data structure that could encode
    both ordinality in time and gap-size into the structure's search metadata may prove
    beneficial, provided that the space costs did not become too onerous.
  \end{paragraph}
  \begin{paragraph}{}
    As demonstrated by the experiments in this work, space cost becomes time cost.
    Determining what information is absolutely essential for an algorithm to function
    and discarding what is extraneous can garner significant time savings.
    The structures used by the original gap-enumeration algorithm would be more amenable
    to whole schedule reconstruction than the techniques presented in this work, but if
    all one needs is a measure of worst case response time for a given task, the benefits
    of the optimizations presented here are undeniable. Furthermore, it would not be
    difficult to maintain an additional schedule structure that accumulated the positive
    fills of the gaps during the execution of the gap-enumeration algorithm.
    Continual, careful optimization of the techniques first explored by Belwal and Cheng
    has the potential to reap increasing benefits within the growing field of
    P-FRP response time analysis.
  \end{paragraph}
\end{section}

\begin{section}{Acknowledgments}
  \begin{paragraph}{}
    The author would like to thank Dr. Albert M. K. Cheng for his introduction to the original work by Chaitanya Belwal and himself.
  \end{paragraph}
\end{section}

\printbibliography

\begin{section}{Supplemental Material}
\end{section}
\end{document}
